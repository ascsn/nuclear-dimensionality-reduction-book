{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e3bd41e",
   "metadata": {},
   "source": [
    "# Why emulators?\n",
    "\n",
    "Contributed by: Pablo Giuliani, Ruchi Garg.\n",
    "\n",
    "Before we dive into the Reduced Basis Method, we first must look inward, deep inside our souls, and ask a fundamentally important question: why would we even want to learn about all this?\n",
    "\n",
    "The Reduced Basis Method, from a practitioner point of view, can be thought of as an emulator, which is an algorithm that can be trained to “mimic” or approximate expensive calculations, usually with very little loss in accuracy, but with a gigantic gain in speed. If setting up the emulator is not too much of a hassle, then in principle we should always make use of them if the accuracy loss is indeed negligible. At the end of the day, performing the same calculation 100 times faster should be a great win for almost every research group. Let us walk you through two short examples, in which the speed up gain could not only be beneficial, but decisive in the amount of science that can be produced.\n",
    "\n",
    "\n",
    "## First case: calibrating and quantifying the uncertainty of a physics model\n",
    "\n",
    "Imagine that you and your colleagues have developed a nice physics model $f(\\alpha,x)$ that depends on some list of parameters $\\alpha$ and explains a phenomena we observe in nature as a function of the parameter $x$. For example, you could be modeling how likely it is for a neutron to interact with a nucleus as a function of the beam energy. The parameters $\\alpha$ could be modeling, for example, how strongly the neutrons interact with other particles in the nucleus.\n",
    "\n",
    "<center><img src=\"Fig1WhyEmu.png\" alt=\"Fig. 1\" style=\"width:320px;height:450px;\"></center>\n",
    "\n",
    "Figure1: Hypothetical situation you find yourself in after starting reading this book. Your model (in red) for a given parameter set $\\alpha$ is compared with experimental data (in blue) your friends from the laboratory just measured.\n",
    "\n",
    "You would like your model to compare well with the recent experimental data your friends have just measured at the laboratory after an expensive and long campaign. For that, you will have to explore the response of your model $f(\\alpha,x)$ for different values of the parameters $\\alpha$ until you find one that matches the observed data well. This can take several evaluations of your model, perhaps of the order of hundreds to several thousands, a task that could become very time consuming if each evaluation takes an appreciable time to compute. Here are two hypotetical examples on opposite sides of the computational burden spectrum:\n",
    "\n",
    "<center><img src=\"Fig2WhyEmu.png\" alt=\"Fig. 2\" style=\"width:450px;height:100px;\"></center>\n",
    "\n",
    "\n",
    "If instead of just finding the set of optimal parameters $\\alpha$ you are interested instead in finding a Bayesian posterior distribution of them to [quantify the uncertainty](https://bandframework.github.io/) of your model, things can quickly get out of hand. \n",
    "\n",
    "<center><img src=\"Fig3WhyEmu.png\" alt=\"Fig. 3\" style=\"width:450px;height:350px;\"></center>\n",
    "\n",
    "Figure 2: Same as Figure 1 with the addition of an uncertainty band in orange. In this hypotetical example, such band was built by sampling 5,000,000 values of the parameter $\\alpha$ and evaluating the model $f(\\alpha,x)$ to compare with the experimental data in blue.\n",
    "\n",
    "These posteriors could sometimes take millions of evaluations to compute. If you were not to run them in parallel in a computer cluster, this will turn the total computational time needed to several years in the case of the simple model, and to several millennia in the case of the complex one. In these cases, an emulator becomes almost a required resource if you would like to finish your uncertainty quantification of your model $f(\\alpha,x)$ within your lifetime.\n",
    "\n",
    "## Second case, experimental optimization and online tune-up\n",
    "\n",
    "Now let’s imagine that you put your experimental hat and you go to the laboratory to join the heroes that carefully measure nature in all its glory. An experimental set up consists of several elements and moving parts that all have to be synchronized almost to perfection to provide reliable data to the scientific community. For example, in the case of a facility that investigates rare nuclei, one part of the experimental set up is completely dedicated to the focusing and control of the beam of particles as they traverse their way from their production, through their target where the reaction occurs, until finally arriving at the target location. Figure 3 illustrates this situation for the Separator for Capture Reactions (SECAR).\n",
    "<center><img src=\"Fig4WhyEmu.png\" alt=\"Fig. 4\" style=\"width:750px;height:450px;\"></center>\n",
    "\n",
    "\n",
    "\n",
    "Figure 3: Adapted from [a recent study](https://doi.org/10.1103/PhysRevAccelBeams.25.044601) by S. A. Miskovich et al., showing the focusing and defocusing of the beam of isotopes as it passes through the dipoles, quadrupoles, and octupoles magnet configurations at SECAR. The final step of the beam path is drawn as a star on a blue target, which will be missed by the beam if the currents passing through each of the magnents in SECAR is not set correctly.\n",
    "\n",
    "The process of finding an optimal setting for the many parameters in the apparatus controlling the beam location can be challenging, sometimes involving the use of computationally expensive simulations and the fine tune of several components by hand. For such challenges, the use of an emulator that can mimic the alignment response and reduce the effective dimensionality in the large parameter space could help tremendously in speeding up the general tunning process. Further speeding up the necessary simulation packages could enable the on-line tunning in real time of the beam location as the experimental conditions are changing during the actual measurement, maximizing the time in which useful data is being actively collected.\n",
    "\n",
    "\n",
    "We hope these two examples have served as an appetizer to motivate you for further reading and studying these techniques. If so, please join us on our next chapter where we give an intuitive description on why any of these types of technique have a chance of working. In a nutshell, most of our computations and calculations involving varying parameters are highly redundant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2419207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
